# Neural Networks Learning
In this exercise we will implement the backpropagation algorithm for neural networks and apply it to the taks of handwritten
digit recognition.

## Neural Networks
In the previous exercise we implemented feedforward propagation to predict handwritten digits using previously learned
weight parameters for each layer in the network. In this exercise, we'll implement the backpropagation algorithm
to generate weights for our model.

## Visualizing the data
Let's go ahead and select a few random images from our training set and plot them
```{r, eval=FALSE}
# Read images in 
X <- as.matrix(read.csv("./data/ex3data1_features.txt", head=FALSE))
idx <- sample(5000, 100)
canvas <- matrix(0, ncol=20*10, nrow=20*10)
samp_images <- X[idx,]
count <- 1
for(i in seq(1,20*10, by=20)){
  for(j in seq(1,20*10,by=20)){
    canvas[i:(i+20-1), j:(j+20-1)] <- matrix(samp_images[count,], nrow=20)
    count <- count + 1
  }
}
image(canvas, col=grey(seq(1,0,length=100)))
```

## Model representation
We went over the network structure while implementing the feedforward algorithm in the previous post ![](http://mlbernauer.bitbucket.org/R/20150228_coursera_ml_neural_networks.html). The network is composed of three laywers; the input layer, single hidden layer and an output layer. The input layer consists of 401 (400 features + 1 bias) nodes, the hidden layer consists of 26 nodes (25 features + 1 bias) and the output layer consists if 10 output nodes corresponding to the class labels of the 10 digits our network should be able to recognize.

## Feedforward and cost function
The cost funciton for this particular neural network is almost identical to that of logistic regression

$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}[-y_{k}^{(i)}log(h_{\theta}(x^{(i)})_{k}) - (1 - y_{k}^{(i)})log(1-(h_{\theta}(x^{(i)}))_{k})]$$
where $h_{\theta}(x^{(i)})$ is computed from the weighted sum of inputs for each node mapped through the sigmoid function. and $K = 10$ which is the number
of class labels.

Similar to the one-vs-all implementation of logistic regression we need to recode the labels, $y$, to a K dimensional vector where $y \in (0,1)$

The feedforward algorithm computes $h_{\theta}(x^{(i)})$ for every example $i$ and summs over all examples

## Implementation
```{r}
# Sigmoid function
sigmoid_function <- function(z){
  1 / (1 + exp(-z))
}

# Feed forward algorithm
feed_forward <- function(X, theta1, theta2){
  m <- dim(X)[1]
  z2 <- X %*% t(theta1)
  a2 <- sigmoid_function(z2)
  a2 <- cbind(rep(1,m), a2)
  z3 <- a2 %*% t(theta2)
  return(sigmoid_function(z3))
}

# Neural net cost function
# y must be converted to logical vector
nn_cost_function <- function(X, y, theta1, theta2){
  m <- dim(X)[1]
  hx <- feed_forward(X, theta1, theta2)
  J <- (1/m) * sum(apply((-y * log(hx) - (1 - y)*log(1 - hx)), 1, sum))
  return(J)
}
```

If `nn_cost_function` has been implemented correctly we should get a cost of about 0.287629 using the learned parameters
from the last post. Let's go ahead and try it out.

```{r}
# Test the cost function
theta1 <- as.matrix(read.csv("./data/ex3theta1.csv", header=F))
theta2 <- as.matrix(read.csv("./data/ex3theta2.csv", header=F))
train_mat <- as.matrix(read.csv("./data/ex3data1_features.txt", header=F))
labels <- as.matrix(read.csv("./data/ex3data1_labs.txt", header=F))
labels <- t(apply(labels, 1, function(x) x == 1:10))

train_mat <- cbind(rep(1,dim(train_mat)[1]), train_mat)

cost <- nn_cost_function(train_mat, labels, theta1, theta2)
cost
```
We see that the cost computed from our implemented cost function is `r cost` which is the expected cost of 0.287629.


## Regularized cost function
Now we should implement regularization to prevent overfitting

$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}[-y_{k}^{(i)}log(h_{\theta}(x^{(i)})_{k}) - (1 - y_{k}^{(i)})log(1-(h_{\theta}(x^{(i)}))_{k})] + 
\frac{\lambda}{2m}[\sum_{j=1}^{25}\sum_{k=1}^{400}(\Theta_{j,k}^{(1)} + \sum_{j=1}^{10}\sum_{k=1}^{25}(\Theta_{j,k}^{(2)}]$$

```{r}
# Cost function with added regularization term
# y must be converted to logical matrix
nn_regularized_cost_function <- function(X, y, theta1, theta2, lambda=1){
  m <- dim(X)[1]
  hx <- feed_forward(X, theta1, theta2)
  reg_term <- (lambda/(2*m)) * (sum(theta1[,-1]^2) + sum(theta2[,-1]^2))
  J <- (1/m) * sum(apply((-y * log(hx) - (1 - y)*log(1 - hx)), 1, sum)) + reg_term
  return(J)
}
```
If the `nn_regularized_cost_function` has been implemented correcly we should see a cost of about 0.383770.
Let's test it

```{r}
# Test the regularized cost function
reg_cost <- nn_regularized_cost_function(train_mat, labels, theta1, theta2, lambda=1)
reg_cost
```

From the output obove we can see our cost, using the previously learned parameters, is `r reg_cost` which is really close to the 
expected 0.383770. This lets us know that we have a working cost function. We are now ready to implement the backpropagation algorithm
in order to learn our own parameters for the network.

## Sigmoid gradient
The backpropagation algorithm depends on the sigmoid gradient. We must write a function to calculate the gradient
of the sigmoid function with respect to $z$
$$g'(z) = \frac{d}{dz}g(z) = g(z)(1-g(z))$$
when z = 0 the gradient should be exactly 0.25 and close to zero for large (both positive and negative) values of $z$.
```{r}
sigmoid_gradient_function <- function(z){
  sigmoid_function(z) * (1 - sigmoid_function(z))
}

sigmoid_gradient_function(0)
sigmoid_gradient_function(-100)
sigmoid_gradient_function(100)
```
From the output above we can determine that the gradient function has been implemented correctly.
## Backpropagation
The intuition behind backpropagation is as follows. Give an training example ($x^{t}, y^{t}$), we will run a forward pass to compute
all the activations throughout the network, including the output value of the hypothesis function $h_{\theta}(x)$. Then for each node $j$
in layer $l$, we will compute an "error term" $\delta_{j}^{(i)}$ that measures how much that node was "responsible" for any errors in the output.
For an output node we can directly measure the difference between the network's activiation and the true target value, and use that to define $\delta_{j}^{(3)}$ 
(since layer 3 is the output layer). For the hidden units, we compute $\delta_{j}^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$.

The backpropagation algorithm will implement steps 1-4 in a _for_ loop for $i = 1:m$k, performing each calculation on the $t^{th}$ training example. The last step
divides the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.

1. Set the input layer's values ($a^{t}$) to the $t$-th training example $x^{(t)}$. Performa a feedforward pass.
2. For each output in unit $k$ in the output layer, set $$\delta_{k}^{(3)} = (a_{k}^{(3)} - y_{k})$$
where $y_{k} \in (0,1)$
3. For the hidden layer $l=2$, set $$\delta^{(2)} = (\Theta^{(2)})^{T}\delta^{(3)}.*g'(z^{(2)})$$
4. Accumulate the gradient from this example using the following formulat. Note that you should skip or remove $\delta_{0}^{(2)}$. 
$$\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^{T}$$

5. obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\frac{1}{m}$
$$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta) = D_{ij}^{(i)} = \frac{1}{m}\Delta_{ij}^{(l)}$$

```{r}
backpropagation <- function(X, y, hidden_nodes=25, out_nodes=10, lambda=1){
  m <- dim(X)[1]
  n <- dim(X)[2]
  # Create theta1
  theta1 <- matrix(0, ncol=n, nrow=hidden_nodes)
  # Create theta2
  theta2 <- matrix(0, ncol=(hidden_nodes+1), nrow=out_nodes)
  grad <- matrix(0, nrow=(hidden_nodes), ncol=1)
  
  for(i in 1:m){
    # 1. Run feed forward to get predictions
    feat <- matrix(X[i,], nrow=1)
    hx <- feed_forward(feat, theta1, theta2)
    d3 <- t(hx - y[i,])
    z2 <- feat %*% t(theta1)
    g <- sigmoid_gradient_function(z2)    
    g <- cbind(1,g)
    d2 <- t(theta2) %*% d3 * t(g)
    grad[,1] <- grad[,1] + d3*sigmoid_function(z2)
    return(d2)
  }
}
```
