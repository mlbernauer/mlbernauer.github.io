<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Regression Week 2: Multiple Linear Regression Quiz 2</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Regression Week 2: Multiple Linear Regression Quiz 2</h1>

<p>Estimating Multiple Regression Coefficients (Gradient Descent)</p>

<p>In this first notebook we explored multiple regression using <code>lm</code>.
Now we will implement our own functions to estimate model parameters
using gradient descent.</p>

<p>In this notebook we will cover estimating multiple linear regression weights
via gradient descent. We will:</p>

<ul>
<li>Add a constant column of 1s to the feature data</li>
<li>Converte these features from a data frame to a matrix</li>
<li>Write a <code>predict_output()</code> function</li>
<li>Write a function to compute the derivative of the regression weights
with respect to a single feature</li>
<li>Write gradient descent function to compute the regression weights given an
initial weigth vector, step size and tolerance</li>
<li>Use the gradient descent function to estimate regression weights for multiple
features</li>
</ul>

<p><strong>1. Load train and test data</strong></p>

<pre><code class="r">train_data = read.csv(&#39;kc_house_train_data.csv&#39;, header=T, sep=&quot;,&quot;)
test_data = read.csv(&#39;kc_house_test_data.csv&#39;, header=T, sep=&quot;,&quot;)
</code></pre>

<p><strong>2. Write a function that take a data set, a list of features to be used as inputs,
and a name of the output (e.g. <code>price</code>). This function should return a <code>features_matrix</code>
consisting of first a column of ones followed by columns containing the values of the 
input features in the data set in the same order as the input list, it should also
return an <code>output_array</code> which is an array of the values of the output i the data set
(e.g. <code>price</code>)</strong></p>

<pre><code class="r">get_matrix_data = function(dataframe, features, output){
    dataframe$constant = 1
    features_matrix = as.matrix(dataframe[,c(&#39;constant&#39;, features)])
    output = as.matrix(dataframe[,c(output)])
    return(list(features_matrix = features_matrix, output=output))
}
</code></pre>

<p><strong>3. If the features matrix (including the column of 1s for the constant) is stored
as a 2D array (matrix) and the regression weights are stored as a 1D array, then the
predicted output is just the dot product between the features matrix and the weights.
Write a function <code>predict_output()</code> which accepts a 2D array <code>feature_matrix</code> and a
1D array <code>weights</code> and returns a 1D array <code>predictions</code></strong></p>

<pre><code class="r">predict_output = function(feature_matrix, weights){
    return(feature_matrix %*% weights)
}
</code></pre>

<p><strong>4. If we have the values of a single input feature in an array <code>feature</code> and the prediction
<code>errors</code> (predictions - output) then the derivative of the regression cost function with
respect to the weight of <code>feature</code> is just wice the dot product between <code>feature</code> and <code>errors</code>.
Write a fuction that accepts a <code>feature</code> array and the <code>error</code> array and return sthe <code>derivative</code>
(a single number).</strong></p>

<pre><code class="r">feature_derivative = function(errors, features){
    return(2 * (t(features) %*% errors))
}
</code></pre>

<p><strong>5. Now we will use <code>predict_output()</code> and <code>feature_derivative()</code> to write a gradient descent
function. Although we can compute the derivative for all the features simultaneously (the
gradient) we will explicitly loop over the features individually for simplicity. Write a
gradient descent function that does the following:</strong> </p>

<ul>
<li>Accepts a <code>feature_matrix</code>, a 1D output array, an array of initial weights, <code>step_size</code>,
and a convergence tolerence.</li>
<li>While not converged updates each feature weight by subtracting the step size times the
derivative for that feature given the current weights</li>
<li>At each step computes the magnitude/length of the gradient (square root of the sum of squared components)</li>
<li>When the magnitude of the gradient is smaller than the input tolerance, return the final weight
vecotr</li>
</ul>

<p>Note: instead of using the function <code>feature_derivative()</code> to compute the gradient for each feature I 
used the formula: </p>

<p>\(\triangledown RSS = -2H^{T}(y-Hw)\)</p>

<p>Where \(H^{T}\) is the transposed feature matrix, \(y\) is the vector of outputs \(H\) is the feature matrix and \(w\) is
the vector of weights. This is gradient equation is derived by taking partial derivatives with respect to each
weight, \(\triangledown RSS\) is a vector of gradients evaluate for a given set of \(w\)</p>

<pre><code class="r">regression_gradient_descent = function(feature_matrix, output, initial_weights, step_size, tolerance){
    converged = FALSE
    weights = initial_weights
    while(!converged){
        predictions = predict_output(feature_matrix, weights)
        error = predictions - output
        gradient = -2*t(feature_matrix) %*% error
        gradient_magnitude = sqrt(sum(gradient**2))
        weights = weights + step_size * gradient
        if(gradient_magnitude &lt; tolerance){
            converged = TRUE
        }
    }
    return(weights)
}
</code></pre>

<p><strong>6. No we will run the <code>regression_gradient_descent</code> function on some acutal data. In
particular we will use the gradient descent to estimate the model from Week 1 using just
an intercept and slope. Use the following parameters:</strong></p>

<ul>
<li>Features: <code>sqft_living</code></li>
<li>Output: <code>price</code></li>
<li><code>initial_weights = [1,-47000]</code></li>
<li><code>step_size = 7e-12</code></li>
<li><code>tolerance = 2.5e7</code></li>
</ul>

<pre><code class="r">simple_features = c(&#39;sqft_living&#39;)
my_output = &#39;price&#39;
out = get_matrix_data(train_data, simple_features, my_output)
initial_weights = c(-47000, 1)
step_size = 7e-12
tolerance = 2.5e7

# Estimate weights using gradient descent
simple_weights = regression_gradient_descent(out$features_matrix, out$output, initial_weights, step_size, tolerance)
simple_weights
</code></pre>

<pre><code>##                    [,1]
## constant    -46999.8872
## sqft_living    281.9121
</code></pre>

<p><strong>7. QUIZ QUESTION: What is the value of the weight for <code>sqft_living</code> the second element of <code>simple_weights</code>
(rounded to 1 decimal place)</strong></p>

<pre><code>## [1] 281.9
</code></pre>

<p><strong>8. Now build a corresponding <code>test_simple_feature_matrix()</code> and <code>test_output()</code> function using <code>test_data</code>. Using
<code>test_simple_feature_matrix()</code> and <code>simple_weights</code> compute the predicted house prices on all the test data.</strong></p>

<pre><code class="r"># Test the simple_weights parameters using test_data
out = get_matrix_data(test_data, c(&#39;sqft_living&#39;), &#39;price&#39;)
predictions = predict_output(out$features_matrix, simple_weights)
</code></pre>

<p><strong>9. QUIZ QUESTION: What is the predicted price for the 1st house in the TEST data set for model 1 (rounded
to the nearest dollar)</strong></p>

<pre><code>## [1] 356134
</code></pre>

<p><strong>10. Now compute the RSS on all test data for this model. REcord the value and store it for later.</strong></p>

<pre><code class="r">simple_rss = sum((test_data$price - predictions)**2)
simple_rss
</code></pre>

<pre><code>## [1] 2.754e+14
</code></pre>

<p><strong>11. Now we will use the gradient descent to fit a model with more than 1 predictor variable (and an intercept).
Use the following parameters:</strong></p>

<ul>
<li><code>features = [&#39;sqft_living&#39;, &#39;sqft_living15&#39;]</code></li>
<li><code>output = &#39;price&#39;</code></li>
<li><code>initial_weights = [-100000, 1, 1,]</code> (intecept, sqft_living, sqft_living_15)</li>
<li><code>step_size = 4e-12</code></li>
<li><code>tolerance=1e9</code></li>
</ul>

<pre><code class="r">data = get_matrix_data(train_data, c(&#39;sqft_living&#39;, &#39;sqft_living15&#39;), &#39;price&#39;)
initial_weights = c(-100000, 1, 1)
step_size = 4e-12
tolerance = 1e9

# Run gradient descent using parameters above
weights = regression_gradient_descent(data$features_matrix, data$output, initial_weights, step_size, tolerance)
weights
</code></pre>

<pre><code>##                       [,1]
## constant      -99999.96885
## sqft_living      245.07260
## sqft_living15     65.27953
</code></pre>

<p><strong>12. Use the regression weights from this second model (using <code>sqft_living</code> and <code>sqft_living_15</code>) and predict
the outcome of all the house prices on TEST data.</strong></p>

<pre><code class="r">data = get_matrix_data(test_data, c(&#39;sqft_living&#39;, &#39;sqft_living15&#39;), &#39;price&#39;)
predictions = predict_output(data$features_matrix, weights)
</code></pre>

<p><strong>13. QUIZ QUESTION: What is the predicted price for the 1st house in the TEST data set for model 2 (round
to the nearest dollar)</strong></p>

<pre><code>## [1] 366651
</code></pre>

<p><strong>14. What is the actual price for the 1st house in the TEST data</strong></p>

<pre><code>## [1] 310000
</code></pre>

<p><strong>15. QUIZ QUESTION: Which estimate was closer to the true price for the 1st house on the TEST data set, model
1 or model 2</strong></p>

<p><strong>16. Now compute RSS on all TEST data for the second model. Record the value and store it for later</strong></p>

<pre><code class="r">second_rss = sum((test_data$price - predictions)**2)
second_rss
</code></pre>

<pre><code>## [1] 2.702634e+14
</code></pre>

<p><strong>17. QUIZ QUESTION: Which model (1 or 2) has lowest RSS on all the TEST DATA</strong></p>

<p>Model 2 with RSS of 2.702634e14</p>

</body>

</html>
