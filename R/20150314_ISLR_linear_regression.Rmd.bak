## Libraries
Librarires, or R-packages are loaded into the R environmen using `library()` or `require()`

```{r}
require(ISLR)
require(MASS)
require(GGally)
require(ggplot2)
```

## Simple linear regression
In this example we will explor th `Boston` dataset contained within the `MASS` package. We will seek to predict
the _medv_ (median house value, in $1000) using _rm_ (average house room number), `age` (average age of houses in neighborhood)
and `lstat` (percentage of households with low socioeconomic status).

```{r}
# First lets explore that Boston dataset
dim(Boston)
names(Boston)
#summary(Boston)

# Plot scatterplo matrix to explore relationships/correlations
ggpairs(Boston[,c("medv", "age", "rm", "lstat")], alpha=0.1)
```

From the scatterplot matrix we can see there is an inverse relationship between `medv` and `lstat`; that is, when
the median value of houses in the neighborhood increases, there is a corresponding drop in the percentage of households
with low socioeconomic status.

```{r}
ggplot(Boston, aes(x=medv, y=lstat)) +
  geom_jitter(alpha=0.7) +
  labs(title="lstat vs medv", x="Median value of homes", y="% homes with low socioeconomic status")
```

From the scatterplot matrix we can also see the positive relationship between `medv` and `rm`

```{r}
ggplot(Boston, aes(x=medv, y=rm)) +
  geom_jitter(alpha=0.7) +
  labs(title="rm vs medv", x="Median value of homes", y="Avg house room number")
```

The correlation between `medv` and `age` is less apparent however, there does seem to be a negative relation ship. It appears
as if the median value of the house decreases slightly with age.

```{r}
ggplot(Boston, aes(x=medv, y=age)) + 
  geom_jitter(alpha=0.7) + 
  labs(title="age vs medv", x="Median value of homes", y="Average age of house")
```

## Linear regression
Now lets start building the linear regression model. First we'll make a model using a
single regression (feature/covariate) and then we'll build the model up by including more
features. To start, lets try regressing `medv` on `rm` and see how that works.

```{r}
fit <- lm(medv ~ lstat, data=Boston)
summary(fit)
confint(fit)

# Check to see how the model fits
intercept <- coef(fit)[1]
slope <- coef(fit)[2]
ggplot(Boston, aes(x=lstat, y=medv)) +
  geom_point(position=position_jitter(), alpha=0.6) + 
  geom_abline(intercept=intercept, slope=slope, col="red")
```

From the summary output we can see that both parameter estimates are significant, that is both the intercept
and the slope have P-values < 0.05. Additionally, the slope parameter is greater than 0 and the confidence intervals
for the slope paramter do not cross 0, suggesting a significant relationship between `rm` and `medv`. In fact, the slope
is 9.102 which suggests that each additional room changes the median value of the house by $9,102.

## Checking assumptions
We can check assumptions about our model such as equal variance by plotting the residuals by calling the `plot()` method. This
will also tell us if our data is non-linear. If the data is non-linear than the residuals will exhibit some sort of pattern
when plotted.

```{r}
par(mfrow=c(2,2))
plot(fit)
```

On the basis of the residuals there appears to be some non-linearity in our data. Additinally, residuals and studentized residuals
can be plotted using `studres(fit)` and `residuals(fit)` respectively. Leverage statistics can be computed using `hatvalues(fit)`.


## Making predicitons 
Once we have a working model we can make predictions using the `predict()` method. In this example we will predict
the median value of houses in a neighborhodo given average room sizes of houses in that neighborhood.

```{r}
# Create a dataframe containing houses with various room sizes
new_data <- data.frame(lstat=c(8, 9, 10, 15, 20, 30))

# Return predictions with confidence intervals 
predict(fit, new_data, interval="confidence")

# Return predictions with prediciton intervals
predict(fit, new_data, interval="prediction")
```

You can see that the predicted `medv` appears to increase by about $9,102 for every one unit increase in `rm`.

## Multiple linear regression
In the above example we created a model to predict `medv` using only a single variable, `rm` however we can build
more complex models by including more regressiors. For example, we could include `age`, `lstat` as well as `rm`
into our model. We could the determine which variables significantly influence (i.e. predict) `medv`.

```{r}
fit <- lm(medv ~ lstat + age, data=Boston)
summary(fit)
```

We can create a model with all 13 variables in `Boston` dataset

```{r}
# We can use . to tell the lm function to use all variables
fit <- lm(medv ~ . , data=Boston)
summary(fit)
```

```{r}
fit_summary <- summary(fit)
fit_summary$r.sq
fit_summary$sigma
```

The `car` package can be used to compute the variance inflation factors

```{r}
library(car)
vif(fit)
```

We can create a model using all but one of the variables

```{r}
# Create a new model with all variables except age
fit <- lm(medv ~ . -age, data=Boston)

# Update a model to include age
fit <- update(fit, ~ . + age, data=Boston)
summary(fit)
```

## Interaction terms
We can include interaction effects between variables by using `rm:lstat` which includes only
the interaction in the model. However, if we use `rm*lstat` the notation will automatically
include the individual variables in the model.

```{r}
fit <- lm(medv ~ age:rm, data=Boston)
fit2 <- lm(medv ~ age*rm, data=Boston)
summary(fit)
summary(fit2)
```

## Non-linear transformations on variables
We've seen that there seems to be a non-linear relationship between
`medv` and `lstat` we can fit a non-linear model to this by performing
some sort of tranformation on the variables.

```{r}
quad_fit <- lm(medv ~ lstat + I(lstat^2), data=Boston)
linear_fit <- lm(medv ~ lstat, data=Boston)
predictions <- data.frame(pred=predict(quad_fit, data.frame(lstat=1:30)))
ggplot(Boston, aes(x=lstat, y=medv)) + geom_point(alpha=0.5) + 
  geom_line(data=predictions, aes(x=1:dim(predictions)[1], y=pred), color="red") + 
  geom_abline(intercept=intercept, slope=slope, color="blue")
```

The plot above shows the two different models; the linear model and the quadratic model it appears
that the non-linear model fits the data better, however to test this more formally using `anova()`

```{r}
# Assess whether the null hypothesis is true, i.e. the two models are the same
anova(quad_fit, linear_fit)
```

Lets inspect the residuals and leverage statistics for the new plot

```{r}
par(mfrow=c(2,2))
plot(quad_fit)
```
Using the `poly()` function will include all lower-order polynomial terms up to the specified order.

```{r}
fit <- lm(medv ~ poly(lstat, 5), data=Boston)
summary(fit)
```

## Qualitative predictors
Qualtative variables can be used in regression models too, to see how R encodes these variables
we can use the `contrasts()` function

```{r}
data(Carseats)
head(Carseats)
contrasts(Carseats$ShelveLoc)
```

Now lets try to predict carseat sales, `Sales` using the features of `Carseats`. We will include
interaction terms for `Income` and `Advertising` as well as `Price` and `Age`

```{r}
fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data=Carseats)
summary(fit)
```



